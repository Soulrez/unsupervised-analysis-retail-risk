{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Data Processing](#processing)\n",
    "2. [Analytic Models](#models)\n",
    "    1. [Regression](#reg)\n",
    "        1. [Model Tuning](#tuning)\n",
    "        2. [Summary of Models](#summary)\n",
    "    2. [Identifying Important Features](#ident)\n",
    "3. [Dimension Reduction and Visualization](#vis)\n",
    "    1. [Principal Component Analysis](#pca)\n",
    "    2. [t-Stochastic Nearest Neighbor Embedding (t-SNE)](#tsne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing <a name=\"processing\"></a>\n",
    "The conversion from Excel Workbook to CSV was done with my old code. The dataset consists of 991 examples with 28 features, and the risk factor of an employee, which we treat as the label. First, we load the CSV file into a numpy array and sort the data by risk factor from low to high. Then, separate the features and the labels (risk factor). Since the features are all on different scales, we'll standardize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the data array: (991, 28)\n",
      "Dimensions of the labels array: (991,)\n",
      "Dimensions of the data array after preprocessing: (991, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/88.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "file = np.loadtxt(open(\"data/safeway/report.csv\", \"rb\"), delimiter=\",\")\n",
    "# Sort data low-to-high based on risk factor\n",
    "file_sorted = file[file[:,0].argsort()]\n",
    "# Split the features from the labels\n",
    "data = file_sorted[:,1:] # Features\n",
    "labels = file_sorted[:,0] # Labels\n",
    "\n",
    "# Sanity check\n",
    "print(\"Dimensions of the data array:\",data.shape)\n",
    "print(\"Dimensions of the labels array:\",labels.shape)\n",
    "data = preprocessing.scale(data)\n",
    "print(\"Dimensions of the data array after preprocessing:\",data.shape)\n",
    "\n",
    "hist = [go.Histogram(x=labels)]\n",
    "layout = go.Layout(\n",
    "    title='Distribution of Risk Factors within Dataset',\n",
    "    xaxis=dict(\n",
    "        title='Risk Factor'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Count'\n",
    "    ),\n",
    ")\n",
    "py.iplot(go.Figure(data=hist, layout=layout), filename='Risk_Facotr_Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train-test sets, 90% train and 10% test\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(data, labels, test_size=0.1, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Models <a name=\"models\"></a>\n",
    "### Regression Models <a name=\"reg\"></a>\n",
    "We use a variety of regression models to predict the numeric risk factor given new features. For performance evaluation, we report the mean squared error, mean absolute error, and R^2 score on the test set for each algorithm. Mathematically, MSE is the (prediction - true value)^2, MAE is abs(prediction - true value), and R^2 is the proportion of the variance in the dependent variable that is predictable from the independent variables. Specifically, we apply Linear/Ridge/Lasso regression, SVM, k-NN, Decision Tree, Random Forest, and Multilayer Perceptron algorithms to this dataset. Some of these models need to be tuned for better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning/Hyperparameter Search <a name=\"tuning\"></a>\n",
    "For ridge and lasso regression, there is a hyperparameter alpha that needs to be tuned. The larger the value of alpha, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. For k-NN, we can try different numbers of neighbors the algorithm considers when making prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, svm, tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Returns MSE, MAE, and R2 score of the model on test set\n",
    "# Parameters:\n",
    "# name: String containing name of the model\n",
    "# model: Object to perform regression with\n",
    "def evaluate_model(name, model):\n",
    "    names, mse, mae, r2 = ([] for i in range(4))\n",
    "    names.append(name)\n",
    "    model.fit(X_train, y_train_reg)\n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "    mse.append('{0:.4}'.format(mean_squared_error(y_test_reg, predicted)))\n",
    "    mae.append('{0:.4}'.format(mean_absolute_error(y_test_reg, predicted)))\n",
    "    r2.append('{0:.4}'.format(r2_score(y_test_reg, predicted)))\n",
    "    return [mse[0], mae[0], r2[0]]\n",
    "\n",
    "# Generates a plot of the tuning curve for MSE, MAE, and R2\n",
    "# Parameters:\n",
    "# name: String containing name of the model\n",
    "# hp_name: String containing the name of the hyperparameter being tuned\n",
    "# hp_values: List of values that the hyperparameter can take on\n",
    "def plot_tuning_curve(name, hp_name, hp_values):\n",
    "    scores = []\n",
    "    # Vary Alphas on a Log-Scale\n",
    "    for a in hp_values:\n",
    "        if name == \"Ridge Regression\":\n",
    "            model = linear_model.Ridge(alpha=a)\n",
    "        if name == \"Lasso Regression\":\n",
    "            model = linear_model.Lasso(alpha=a)\n",
    "        if name == \"k-NN\":\n",
    "            model = KNeighborsRegressor(n_neighbors=a)\n",
    "        if name == \"MLP\":\n",
    "            model = MLPRegressor(activation ='logistic', hidden_layer_sizes=(5, ), max_iter=50000, solver='lbfgs', alpha=a)\n",
    "        scores.append(evaluate_model(\"%s (a=%s)\" % (name, a), model))\n",
    "        \n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    mse = go.Scatter(\n",
    "        x = hp_values,\n",
    "        y = scores[:,0],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'MSE'\n",
    "    )\n",
    "\n",
    "    mae = go.Scatter(\n",
    "        x = hp_values,\n",
    "        y = scores[:,1],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'MAE'\n",
    "    )\n",
    "\n",
    "    r2 = go.Scatter(\n",
    "        x = hp_values,\n",
    "        y = scores[:,2],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'R2'\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Tuning %s Varying %s' % (name, hp_name),\n",
    "        xaxis=dict(\n",
    "            title='%s (Log-Scale)' % hp_name,\n",
    "            type='log'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Value'\n",
    "        ),\n",
    "    )\n",
    "    scatter = [mse, mae, r2]\n",
    "    return go.Figure(data=scatter, layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/94.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_tuning_curve(\"Ridge Regression\", \"Alpha\", [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500]), filename='Ridge_Regression_Alpha_Tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/96.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_tuning_curve(\"Lasso Regression\", \"Alpha\", [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500]), filename='Lasso_Regression_Alpha_Tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/98.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_tuning_curve(\"k-NN\", \"Number of Neighbors\", [1, 5, 10, 32, 50, 100, 500]), filename='kNN_Tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/102.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_tuning_curve(\"MLP\", \"Alpha\", [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000]), filename='MLP_Alpha_Tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Models <a name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/90.embed\" height=\"350px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report the MSE, MAE, and R2 values for all models\n",
    "names = np.array([\"Linear Regression\", \"Lasso Regression\", \"Ridge Regression\", \"SVM RBF Kernel\",\"SVM Linear Kernel\", \"k-NN\", \"Decision Tree\", \"Random Forest\", \"Multilayer Perceptron\"])\n",
    "report = []\n",
    "\n",
    "# Linear regression/ordinary least squares\n",
    "linreg = linear_model.LinearRegression()\n",
    "report.append(evaluate_model(\"Linear Regression\", linreg))\n",
    "\n",
    "# Lasso Regression, aka L1 regularized\n",
    "lasso = linear_model.Lasso(alpha=0.1)\n",
    "report.append(evaluate_model(\"Lasso Regression\", lasso))\n",
    "\n",
    "# Ridge Regression, aka L2 regularized\n",
    "ridge = linear_model.Ridge(alpha=50)\n",
    "report.append(evaluate_model(\"Ridge Regression\", ridge))\n",
    "\n",
    "# Support Vector Regressor\n",
    "svr = svm.SVR(kernel='rbf')\n",
    "report.append(evaluate_model(\"SVM RBF Kernel\", svr))\n",
    "\n",
    "# Support Vector Regressor\n",
    "lsvr = svm.LinearSVR()\n",
    "report.append(evaluate_model(\"SVM Linear Kernel\", lsvr))\n",
    "\n",
    "# k-Nearest Neighbors using k=32 (usually use sqrt(number of examples))\n",
    "knn = KNeighborsRegressor(n_neighbors=10)\n",
    "report.append(evaluate_model(\"k-NN (32)\", knn))\n",
    "\n",
    "# Decision Tree\n",
    "dt = tree.DecisionTreeRegressor()\n",
    "report.append(evaluate_model(\"Decision Tree\", dt))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(max_depth=5)\n",
    "report.append(evaluate_model(\"Random Forest\", rf))\n",
    "\n",
    "# Perceptron\n",
    "mlp = MLPRegressor(activation ='logistic', hidden_layer_sizes=(5, ), max_iter=50000, solver='lbfgs', alpha=0.1)\n",
    "report.append(evaluate_model(\"Multilayer Perceptron\", mlp))\n",
    "\n",
    "report = np.array(report)\n",
    "\n",
    "evaluations = {'Models': names, 'MSE': report[:,0], 'MAE': report[:,1], 'R2': report[:,2]}\n",
    "df = pd.DataFrame(evaluations)[['Models', 'MSE', 'MAE', 'R2']]\n",
    "\n",
    "table = ff.create_table(df)\n",
    "py.iplot(table, filename='Regression_Comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Important Features <a name=\"ident\"></a>\n",
    "Since this data is high dimensional, it's best to identify which features are more important for the predictor. One way to identify the important features is by looking at the coefficients learned by linear models (linear, lasso, and ridge regression). Intuitively, coefficients of features that have larger magnitude are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = [\"Total Number Of Transactions\",\"Total Items Count\",\"Total Sales Amount\",\"Coupons outside of orders\",\"Coupons - Highest Number Of Uses Of Promotion Code\",\"Coupons - Percentage Of Total Amount\",\"Coupons Total Amount\",\"Coupons Count\",\"\",\"Coupons Average Amount\",\"Coupons - Percentage Of All Transactions\",\"Refunds Trx Count\",\"Refunds Item Count\",\"Refunds Total Amount\",\"Department Refunds Percentage Of Total Amount\",\"Department Refunds Total Amount\",\"Department Refunds Percentage Of Items\",\"Department Refunds Item Count\",\"Item Voids Item Count\",\"Item Voids Total Amount Voided\",\"Department Sales Percentage Of All Items\",\"Department Sales Item Count\",\"Department Sales Total Amount\",\"Zero Trx Percentage\",\"Zero Count\",\"Zero Trx With Alcohol\",\"Payment Card Relationship Max No Of Card Uses\",\"No Sales Per Day\",\"Base Avg Basket Size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression: [21 17  8 24 22  5  0 12 20 25 16 14 13 18 15 26  4 19 10 23 11  9 27  3  7\n",
      "  6  2  1]\n",
      "Lasso regression: [27 18 15 26 12 22 20  8 21 24  5  4  2 23 14  1 19 17 25 10 16  9  0 13 11\n",
      "  3  6  7]\n",
      "Ridge regression: [22  8 21  4  5 12 17 24 18 20 14 15 16 26 25  2 19 23 27 13 10  0 11  9  1\n",
      "  7  3  6]\n"
     ]
    }
   ],
   "source": [
    "lin_c = np.absolute(linreg.coef_).argsort()\n",
    "lasso_c = np.absolute(lasso.coef_).argsort()\n",
    "ridge_c = np.absolute(ridge.coef_).argsort()\n",
    "print(\"Linear regression:\", lin_c)\n",
    "print(\"Lasso regression:\", lasso_c)\n",
    "print(\"Ridge regression:\", ridge_c)\n",
    "#print(\"Random Forest:\", rf.feature_importances_.argsort())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually do better than just using the coefficients. Recursive feature elimination will train a model on subsets of the features by pruning the least important features determined by the model's coefficients. This recursive process will help us narrow down the top N features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive feature elimination\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "rfe = RFE(linreg, n_features_to_select=2)\n",
    "rfe.fit(data, labels)\n",
    "linreg_rfe = rfe.ranking_.argsort()\n",
    "\n",
    "rfe = RFE(lasso, n_features_to_select=2)\n",
    "rfe.fit(data, labels)\n",
    "lasso_rfe = rfe.ranking_.argsort()\n",
    "\n",
    "rfe = RFE(ridge, n_features_to_select=2)\n",
    "rfe.fit(data, labels)\n",
    "ridge_rfe = rfe.ranking_.argsort()\n",
    "\n",
    "rfe = RFE(lsvr, n_features_to_select=2)\n",
    "rfe.fit(data, labels)\n",
    "svr_rfe = rfe.ranking_.argsort()\n",
    "\n",
    "rfe = RFE(dt, n_features_to_select=2)\n",
    "rfe.fit(data, labels)\n",
    "dt_rfe = rfe.ranking_.argsort()\n",
    "\n",
    "rfe = RFE(rf, n_features_to_select=2)\n",
    "rfe.fit(data, labels)\n",
    "rf_rfe = rfe.ranking_.argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the top 2 ranking features selected by RFE for each of the regression models, decision tree, and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# name: String containing name of the model\n",
    "# rfe_array: Ranking of features produced by RFE\n",
    "# Return scatter plot of the top two ranked features\n",
    "def generate_RFE_plot(name, rfe_array):\n",
    "    trace1 = go.Scatter(\n",
    "        x = data[:,rfe_array[0]],\n",
    "        y = data[:,rfe_array[1]],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size='7',\n",
    "            opacity=0.75,\n",
    "            color = labels, #set color equal to a variable\n",
    "            colorscale='Portland',\n",
    "            showscale=True,\n",
    "            line = dict(\n",
    "                width = 0.5\n",
    "            )\n",
    "        ),\n",
    "        text=[\"Risk: %s\" %l for l in labels]\n",
    "    )\n",
    "\n",
    "    layout= go.Layout(\n",
    "        title= 'Top 2 Features Ranked by RFE for '+name,\n",
    "        hovermode= 'closest',\n",
    "        xaxis= dict(\n",
    "            title= feature_descriptions[rfe_array[0]]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title= feature_descriptions[rfe_array[1]]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace1], layout=layout)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/108.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(generate_RFE_plot(\"Linear Regression\", linreg_rfe), filename='Top 2 Features Ranked by RFE for Linear Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/110.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(generate_RFE_plot(\"Lasso Regression\", lasso_rfe), filename='Top 2 Features Ranked by RFE for Lasso Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/112.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(generate_RFE_plot(\"Ridge Regression\", ridge_rfe), filename='Top 2 Features Ranked by RFE for Ridge Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/114.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(generate_RFE_plot(\"SVM\", svr_rfe), filename='Top 2 Features Ranked by RFE for Support Vector Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/116.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(generate_RFE_plot(\"Random Forest\", rf_rfe), filename='Top 2 Features Ranked by RFE for Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension-Reduction and Visualization <a name=\"vis\"></a>\n",
    "### Principal Component Analysis <a name=\"pca\"></a>\n",
    "Using feature selection/RFE, we lose out on potentially a lot of relationships between the many features. Another way to represent our data in two or three dimensions is by using PCA. Sometimes using only the principal components will have more predictive power for the model, so we try linear regression with the top 2 PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results with PCA\n",
      "Mean squared error: 2.18\n",
      "Mean absolute error: 1.17\n",
      "Coefficient of determination: -0.01\n"
     ]
    }
   ],
   "source": [
    "# Run PCA on data to reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "data_pca = pca.transform(data)\n",
    "\n",
    "# Train-test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_pca, labels, test_size=0.1, random_state=15)\n",
    "\n",
    "# Train Linear Regression\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, predicted)\n",
    "MAE = mean_absolute_error(y_test, predicted)\n",
    "R2 = r2_score(y_test, predicted)\n",
    "\n",
    "print(\"Linear Regression Results with PCA\")\n",
    "print(\"Mean squared error: %.2f\" % MSE)\n",
    "print(\"Mean absolute error: %.2f\" % MAE)\n",
    "print('Coefficient of determination: %.2f' % R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/118.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot PCA points\n",
    "trace1 = go.Scatter(\n",
    "    x = data_pca[:,0],\n",
    "    y = data_pca[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size='7',\n",
    "        opacity=0.75,\n",
    "        color = labels, #set color equal to a variable\n",
    "        colorscale='Portland',\n",
    "        showscale=True,\n",
    "        line = dict(\n",
    "            width = 0.5\n",
    "        )\n",
    "    ),\n",
    "    text=[\"Risk: %s\" %l for l in labels]\n",
    ")\n",
    "\n",
    "layout= go.Layout(\n",
    "    title= 'Projection of data using first two prinicpal components',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title=\"PC 1\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= \"PC 2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace1], layout=layout)\n",
    "py.iplot(fig, filename='PCA_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/122.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(data)\n",
    "data_pca3 = pca.transform(data)\n",
    "\n",
    "trace1 = go.Scatter3d(\n",
    "    x = data_pca3[:,0],\n",
    "    y = data_pca3[:,1],\n",
    "    z = data_pca3[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size='3',\n",
    "        opacity=0.75,\n",
    "        color = labels, #set color equal to a variable\n",
    "        colorscale='Portland',\n",
    "        showscale=True,\n",
    "    ),\n",
    "    text=[\"Risk: %s\" %l for l in labels]\n",
    ")\n",
    "\n",
    "layout= go.Layout(\n",
    "    title= 'Projection of data using first three prinicpal components',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title=\"PC 1\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= \"PC 2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace1], layout=layout)\n",
    "py.iplot(fig, filename='PCA_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Stochastic Nearest Neighbor Embedding (t-SNE) <a name=\"tsne\"></a>\n",
    "Another method to visualize high dimensional data is by using data embedding techniques (PCA and LDA are special cases of these methods). A method that works particularly well on images and have previously been shown to work better than PCA for biology data is t-SNE.\n",
    "\n",
    "The main challenge with t-SNE and PCA visualizations on this dataset is assigning an objective evaluation of how well the resulting visualizations are. We can calculate the silhouette score, which is used for evaluating clustering algorithms. To do so, we bin our data into N different classes. First, we run k-Means on the original dataset to determine the best number of classes to use. We try a range of k values, and choose the best silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2 The average silhouette_score is : 0.718650809074\n",
      "For n_clusters = 3 The average silhouette_score is : 0.179101457472\n",
      "For n_clusters = 4 The average silhouette_score is : 0.133338710476\n",
      "For n_clusters = 5 The average silhouette_score is : 0.13100932328\n",
      "For n_clusters = 6 The average silhouette_score is : 0.138889684897\n",
      "For n_clusters = 7 The average silhouette_score is : 0.141661654502\n",
      "For n_clusters = 8 The average silhouette_score is : 0.143445754489\n",
      "For n_clusters = 9 The average silhouette_score is : 0.106036357711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "range_n_clusters = range(2,10)\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = kmeans.fit_predict(data)\n",
    "    silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best N to use is 2 (i.e. binary classification: risky or not). We split the data into two bins, and then use t-SNE to transform the dataset. Lastly, we compare the resulting embedding with the assigned classes and compute the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(991,)\n"
     ]
    }
   ],
   "source": [
    "# Originally I thought to use 5 classes, but binary classification will work better\n",
    "#n, bins, patches = plt.hist(labels, 5)\n",
    "#class_dict = [\"Low\", \"Low-Medium\", \"Medium\", \"Medium-High\", \"high\"]\n",
    "#class_labels = np.array([])\n",
    "#for i in range(n.shape[0]):\n",
    "#    labels_ = np.full(int(n[i]), class_dict[i])\n",
    "#    class_labels = np.concatenate((class_labels,labels_))\n",
    "#print(class_labels.shape)\n",
    "\n",
    "labels_low = np.full(496, \"Low\")\n",
    "labels_high = np.full(495, \"High\")\n",
    "class_labels = np.concatenate((labels_low, labels_high))\n",
    "print(class_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the t-SNE cost function is non-convex, we will get different results from different initializations. Therefore, we have to tune the perplexity and learning rate parameters. Additionally, running it for longer iterations will also result in different embeddings. We can also be selective about the number of features we use, so we also try the top N features produced from RFE for random forest. We then tune the model based on its silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Takes up to 12 hours to run\n",
    "'''\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "perplexity = [2, 5, 30, 50, 100]\n",
    "learning_rate = [10, 100, 1000]\n",
    "iters = [250, 500, 1000, 2000]\n",
    "features = rf_rfe # Use random forest RFE ranking\n",
    "hp_dict_2d_fe = {}\n",
    "j = 0\n",
    "for num_features in range(3, len(features)+1):\n",
    "    new_data = data[:,features[0]][:,np.newaxis]\n",
    "    for i in range(1,num_features):\n",
    "        new_data = np.hstack((new_data, data[:,features[i]][:,np.newaxis]))\n",
    "    for p in perplexity:\n",
    "        for lr in learning_rate:\n",
    "            for i in iters:\n",
    "                print(j)\n",
    "                j += 1\n",
    "                data_embedded = TSNE(n_components=2,perplexity=p,learning_rate=lr,n_iter=i).fit_transform(new_data)\n",
    "                score = silhouette_score(data_embedded, class_labels)\n",
    "                hp = str(num_features)+\"_\"+str(p)+\"_\"+str(lr)+\"_\"+str(i)\n",
    "                hp_dict_2d_fe[hp] = score\n",
    "                plt.figure(figsize=(15,15))\n",
    "                plt.scatter(data_embedded[:,0], data_embedded[:,1], c=labels, cmap=cm.hot_r, s=50*np.ones(991), alpha=0.5)\n",
    "                plt.colorbar()\n",
    "                plt.savefig(\"figures_feature_elim/\"+hp+\".png\", bbox_inches='tight')\n",
    "                plt.close()\n",
    "np.save(\"hp_dict_2d_fe.npy\", hp_dict_2d_fe)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Since I saved the dictionary with numpy, it's loaded as a feature array rather than a dictionary. \n",
    "# So I manually it convert it back here.\n",
    "hp_dict = np.load(\"hp_dict_2d_fe.npy\")\n",
    "new_dict = {}\n",
    "for key in hp_dict.item():\n",
    "    new_dict[key] = hp_dict.item().get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 t-SNE hyperparameter settings and silhouette scores\n",
      "('4_5_10_250', 0.14942884)\n",
      "('3_30_10_1000', 0.13991536)\n",
      "('3_30_1000_500', 0.13929713)\n",
      "('3_30_100_500', 0.1388244)\n",
      "('3_30_10_2000', 0.13852572)\n",
      "('3_30_10_500', 0.13746214)\n",
      "('3_50_10_500', 0.13647822)\n",
      "('3_5_100_250', 0.13642122)\n",
      "('3_30_100_1000', 0.13638541)\n",
      "('3_30_1000_1000', 0.13621932)\n",
      "Compared against PCA silhouette score\n",
      "('PCA', 0.071961015092428415)\n"
     ]
    }
   ],
   "source": [
    "# todo: turn this into a dataframe/plotly table\n",
    "best = Counter(new_dict)\n",
    "print(\"Top 10 t-SNE hyperparameter settings and silhouette scores\")\n",
    "for pair in best.most_common(10):\n",
    "    print(pair)\n",
    "print(\"Compared against PCA silhouette score\")\n",
    "print((\"PCA\", silhouette_score(data_pca, class_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the top 5 t-SNE embeddings, and run k-means (k=2) to identify the clusters in the embeddings for a full visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_with_centroids(hp_setting, feature_rankings):\n",
    "    new_data = data[:,feature_rankings[0]][:,np.newaxis]\n",
    "    for i in range(1,int(hp_setting[0])):\n",
    "        new_data = np.hstack((new_data, data[:,feature_rankings[i]][:,np.newaxis]))\n",
    "    data_embedded = TSNE(random_state=10, n_components=2,perplexity=int(hp_setting[1]),learning_rate=int(hp_setting[2]),n_iter=int(hp_setting[3])).fit_transform(new_data)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=10).fit(data_embedded)\n",
    "    centers = np.array(kmeans.cluster_centers_)\n",
    "    \n",
    "    tsne = go.Scatter(\n",
    "        name = \"Data Embedding\",\n",
    "        x = data_embedded[:,0],\n",
    "        y = data_embedded[:,1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size='7',\n",
    "            opacity=0.75,\n",
    "            color = labels, #set color equal to a variable\n",
    "            colorscale='Portland',\n",
    "            showscale=True,\n",
    "            line = dict(\n",
    "                width = 0.5\n",
    "            )\n",
    "        ),\n",
    "        text=[\"Risk: %s\" %l for l in labels]\n",
    "    )\n",
    "    centroids = go.Scatter(\n",
    "        name = \"Centroids\",\n",
    "        x = centers[:,0],\n",
    "        y = centers[:,1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size='15',\n",
    "            symbol= \"x\",\n",
    "            color = \"black\"\n",
    "            )\n",
    "    )\n",
    "    layout= go.Layout(\n",
    "        title= 't-SNE Embedding using %s Features, Perplexity=%s, Learning Rate=%s, Iters=%s' % tuple(hp_setting),\n",
    "        hovermode= 'closest',\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[tsne, centroids], layout=layout)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/124.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(tsne_with_centroids(\"4_5_10_250\".split(\"_\"),rf_rfe), filename=\"4_5_10_250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/126.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(tsne_with_centroids(\"3_30_10_1000\".split(\"_\"),rf_rfe), filename=\"3_30_10_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/128.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(tsne_with_centroids(\"3_30_1000_500\".split(\"_\"),rf_rfe), filename=\"3_30_1000_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/130.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(tsne_with_centroids(\"3_30_100_500\".split(\"_\"),rf_rfe), filename=\"3_30_100_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/132.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(tsne_with_centroids(\"3_30_10_2000\".split(\"_\"),rf_rfe), filename=\"3_30_10_2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is important for visualization. Here, we plot the average silhouette score for t-SNE embeddings across different hyperparameter settings varying the number of features used. As more features are used, the lower the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Soulrez/120.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the mean silhouette score across number of features used\n",
    "mean = {}\n",
    "for i in range(3,29):\n",
    "    results = [v for k,v in new_dict.items() if k.startswith(str(i))]\n",
    "    mean[i] = np.mean(np.array(results))\n",
    "\n",
    "mean = go.Scatter(\n",
    "    x = list(mean.keys()),\n",
    "    y = list(mean.values()),\n",
    "    mode = 'lines+markers',\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Mean Silhouette Score Across Number of Features Used',\n",
    "    xaxis=dict(\n",
    "        title='Number of Features',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Mean Silhouette Score'\n",
    "    ),\n",
    ")\n",
    "scatter = [mean]\n",
    "fig = go.Figure(data=scatter, layout=layout)\n",
    "py.iplot(fig, filename='Mean_Silhouette_Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude from paper (just for exploration)\n",
    "## Classification Models\n",
    "I tried treating this problem as a classification task, just to see how well it would perform. Turns out regression is definitely more suitable.\n",
    "\n",
    "### Assigning Classes/Labels to Examples\n",
    "First, assign each example into a class to perform classification later. For the risk factors, min is 4.57 and max is 16, and we split it evenly into 5 bins using a histogram (AKA equal-width binning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histogram of the data, split into 5 bins\n",
    "n, bins, patches = plt.hist(labels, 5, facecolor='green', alpha=0.75)\n",
    "print(\"Number of elements per bin:\", n) # Number of elements in each bin\n",
    "\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Risk Factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is poorly distributed if we split it evenly based on width. So now, we try equal-frequency binning to split into 5 bins with the same number of elements each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Low\", \"Low-Medium\", \"Medium\", \"Medium-High\", \"high\"]\n",
    "lows = np.full(991//5+1, class_labels[0])\n",
    "lms = np.full(991//5, class_labels[1])\n",
    "meds = np.full(991//5, class_labels[2])\n",
    "mhs = np.full(991//5, class_labels[3])\n",
    "highs = np.full(991//5, class_labels[4])\n",
    "\n",
    "classes = np.concatenate((lows,lms,meds,mhs,highs))\n",
    "print(\"Dimensions of the classes array:\", classes.shape) # Sanity check to make sure still 991\n",
    "\n",
    "n, bins, patches = plt.hist(classes, 5, facecolor='green', alpha=0.75)\n",
    "print(\"Number of elements per bin:\", n) # Number of elements in each bin\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we train an SVM using linear (parametric) and RBF (non-parametric) kernels on the data. For performance evaluation, we output the test set's total precision, recall, and F1 score, and for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train-test sets, 90% train and 10% test\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(data, classes, test_size=0.1, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Linear Support Vector Classifier\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train_class)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test_class, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Support Vector Classifier\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train_class)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test_class, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
